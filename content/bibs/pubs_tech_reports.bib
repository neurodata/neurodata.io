@article {Chandrashekhar2021.01.26.428355,
	author = {Chandrashekhar, Vikram and Tward, Daniel J and Crowley, Devin and Crow, Ailey K and Wright, Matthew A and Hsueh, Brian Y and Gore, Felicity and Machado, Timothy A and Branch, Audrey and Rosenblum, Jared S and Deisseroth, Karl and Vogelstein, Joshua T},
	title = {CloudReg: Automatic Terabyte-Scale Cross-Modal Brain Volume Registration},
	elocation-id = {2021.01.26.428355},
	year = {2021},
	doi = {10.1101/2021.01.26.428355},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Quantifying terabyte-scale multi-modal human and animal imaging data requires scalable analysis tools. We developed CloudReg, an open-source, automatic, terabyte-scale, cloud-based image analysis pipeline that pre-processes and registers cross-modal volumetric datasets with artifacts via spatially-varying polynomial intensity transform. CloudReg accurately registers the following datasets to their respective atlases: in vivo human and ex vivo macaque brain magnetic resonance imaging, ex vivo mouse brain micro-computed tomography, and cleared murine brain light-sheet microscopy.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2021/01/27/2021.01.26.428355},
	eprint = {https://www.biorxiv.org/content/early/2021/01/27/2021.01.26.428355.full.pdf},
	journal = {bioRxiv}
}


@article{graphyti2019,
 author = {Mhembere, Disa and Zheng, Da and Vogelstein, Joshua T. and Priebe, Carey E. and Burns, Randal},
 author+an = {1=trainee;2=trainee;3=highlight},
 journal = {arXiv},
 month = {July},
 title = {Graphyti: A Semi-External Memory Graph Library for FlashGraph},
 url = {https://arxiv.org/abs/1907.03335},
 year = {2019}
}

@article{kiar2017comprehensive,
 author = {Kiar, Gregory and Bridgeford, Eric and Chandrashekhar, Vikram and Mhembere, Disa and Burns, Randal and Roncal, William R Gray and Vogelstein, Joshua T},
 author+an = {1=trainee;2=trainee;3=trainee;4=trainee;6=trainee;7=highlight},
 journal = {bioRxiv},
 month = {Sep},
 pages = {188706},
 title = {A comprehensive cloud framework for accurate and reliable human connectome estimation and meganalysis},
 url = {https://doi.org/10.1101/188706},
 year = {2017}
}

@article{Kiar2018,
 abstract = {Modern scientific discovery depends on collecting large heterogeneous datasets with many sources of variability, and applying domain-specific pipelines from which one can draw insight or clinical utility. For example, macroscale connectomics studies require complex pipelines to process raw functional or diffusion data and estimate connectomes. Individual studies tend to customize pipelines to their needs, raising concerns about their reproducibility, and adding to a longer list of factors that may differ across studies (including sampling, experimental design, and data acquisition protocols), resulting in failures to replicate. Mitigating these issues requires multi-study datasets and the development of pipelines that can be applied across them. We developed NeuroData's MRI to Graphs (NDMG) pipeline using several functional and diffusion studies, including the Consortium for Reliability and Reproducibility, to estimate connectomes. Without any manual intervention or parameter tuning, NDMG ran on 25 different studies ($\sim$6,000 scans) from 15 sites, with each scan resulting in a biologically plausible connectome (as assessed by multiple quality assurance metrics at each processing stage). For each study, the connectomes from NDMG are more similar within than across individuals, indicating that NDMG is preserving biological variability. Moreover, the connectomes exhibit near perfect consistency for certain connectional properties across every scan, individual, study, site, and modality; these include stronger ipsilateral than contralateral connections and stronger homotopic than heterotopic connections. Yet, the magnitude of the differences varied across individuals and studies - much more so when pooling data across sites, even after controlling for study, site, and basic demographic variables (i.e., age, sex, and ethnicity). This indicates that other experimental variables (possibly those not measured or reported) are contributing to this variability, which if not accounted for can limit the value of aggregate datasets, as well as expectations regarding the accuracy of findings and likelihood of replication. We, therefore, provide a set of principles to guide the development of pipelines capable of pooling data across studies while maintaining biological variability and minimizing measurement error. This open science approach provides us with an opportunity to understand and eventually mitigate spurious results for both past and future studies.},
 author = {Kiar, Gregory and Bridgeford, Eric and Roncal, Will Gray and (CoRR) and Chandrashekhar, Vikram and Mhembere, Disa and Ryman, Sephira and Zuo, Xi-Nian and Marguiles, Daniel S and Craddock, R Cameron and Priebe, Carey E and Jung, Rex and Calhoun, Vince and Caffo, Brian and Burns, Randal and Milham, Michael P and Vogelstein, Joshua},
 author+an = {1=trainee;2=trainee;3=trainee;5=trainee;6=trainee;17=highlight},
 doi = {10.1101/188706},
 journal = {bioRxiv},
 month = {apr},
 publisher = {Cold Spring Harbor Laboratory},
 title = {{A High-Throughput Pipeline Identifies Robust Connectomes But Troublesome Variability}},
 url = {https://doi.org/10.1101/188706},
 year = {2018}
}



@article{Statconnect2020,
  title={Statistical Connectomics},
  author={Jaewon Chung and Eric Bridgeford and Jesus Arroyo and Benjamin D. Pedigo and Ali Saad-Eldin and Vivek Gopalakrishnan and Liang Xiang and Carey E. Priebe and Joshua Vogelstein},
  author+an={1=trainee; 2=trainee; 3=trainee; 4=trainee; 5=trainee; 6=trainee; 9=highlight},
  year={2020},
  month = {oct},
  journal={arXiv},
  url={https://osf.io/ek4n3},

}


@article{vogelstein2020pvalues,
      title={P-Values in a Post-Truth World}, 
      author={Joshua T. Vogelstein},
      author+an={1=highlight},
      year={2020},
      month = {jul},
      eprint={2007.03611},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph},
      url={https://arxiv.org/abs/2007.03611}
}




@article{wang2020statistical,
      title={Statistical Analysis of Data Repeatability Measures}, 
      author={Zeyi Wang and Eric Bridgeford and Shangsi Wang and Joshua T. Vogelstein and Brian Caffo},
      author+an={2=trainee;4=highlight},
      year={2020},
      eprint={2005.11911},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/2005.11911}
}


@article{golland2020new,
      title={A New Age of Computing and the Brain}, 
      author={Polina Golland and Jack Gallant and Greg Hager and Hanspeter Pfister and Christos Papadimitriou and Stefan Schaal and Joshua T. Vogelstein},
      author+an={7=highlight},
      year={2020},
      month={apr},
      eprint={2004.12926},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2004.12926}
}





@article{Banerjee2013,
 abstract = {An extremely common bottleneck encountered in statistical learning algorithms is inversion of huge covariance matrices, examples being in evaluating Gaussian likelihoods for a large number of data points. We propose general parallel algorithms for inverting positive definite matrices, which are nearly rank deficient. Such matrix inversions are needed in Gaussian process computations, among other settings, and remain a bottleneck even with the increasing literature on low rank approximations. We propose a general class of algorithms for parallelizing computations to dramatically speed up computation time by orders of magnitude exploiting multicore architectures. We implement our algorithm on a cloud computing platform, providing pseudo and actual code. The algorithm can be easily implemented on any multicore parallel computing resource. Some illustrations are provided to give a flavor for the gains and what becomes possible in freeing up this bottleneck.},
 archiveprefix = {arXiv},
 arxivid = {1312.1869},
 author = {Banerjee, Anjishnu and Vogelstein, Joshua and Dunson, David},
 author+an = {2=highlight},
 eprint = {1312.1869},
 journal = {arXiv},
 keywords = {big data,gaussian process,mapreduce,matrix inversion,parallel computing,qr decom-},
 title = {{Parallel inversion of huge covariance matrices}},
 url = {http://arxiv.org/abs/1312.1869},
 volume = {1312.1869},
 year = {2013}
}

@article{Hayden2020,
 author = {Helm, Hayden S. and Basu, Amitabh and Athreya, Avanti and Park, Youngser and Vogelstein, Joshua T. and Winding, Michael and Zlatic, Marta and Cardona, Albert and Bourke, Patrick and Larson, Jonathan and White, Chris and Priebe, Carey E.},
 author+an = {1=trainee; 5=highlight},
 archiveprefix = {arXiv},
 arxivid = {2005.10700},
 eprint = {2005.10700},
 title = {Learning to rank via combining representations},
 url = {https://arxiv.org/abs/2005.10700},
 year = {2020},
 month={may}
}



@article{Kazhdan2013,
 abstract = {We propose a new gradient-domain technique for processing registered EM image stacks to remove the inter-image discontinuities while preserving intra-image detail. To this end, we process the image stack by first performing anisotropic diffusion to smooth the data along the slice axis and then solving a screened-Poisson equation within each slice to re-introduce the detail. The final image stack is both continuous across the slice axis (facilitating the tracking of information between slices) and maintains sharp details within each slice (supporting automatic feature detection). To support this editing, we describe the implementation of the first multigrid solver designed for efficient gradient domain processing of large, out-of-core, voxel grids.},
 archiveprefix = {arXiv},
 arxivid = {1310.0041},
 author = {Kazhdan, Michael and Burns, Randal and Kasthuri, Bobby and Lichtman, Jeff and Vogelstein, Jacob and Vogelstein, Joshua},
 author+an = {6=highlight},
 eprint = {1310.0041},
 journal = {arXiv},
 month = {sep},
 title = {{Gradient-Domain Processing for Large EM Image Stacks}},
 url = {http://arxiv.org/abs/1310.0041},
 year = {2013}
}

@article{kiar2018neurostorm,
 abstract = {Neuroscientists are now able to acquire data at staggering rates across spatiotemporal scales. However, our ability to capitalize on existing datasets, tools, and intellectual capacities is hampered by technical challenges. The key barriers to accelerating scientific discovery correspond to the FAIR data principles: findability, global access to data, software interoperability, and reproducibility/re-usability. We conducted a hackathon dedicated to making strides in those steps. This manuscript is a technical report summarizing these achievements, and we hope serves as an example of the effectiveness of focused, deliberate hackathons towards the advancement of our quickly-evolving field.},
 archiveprefix = {arXiv},
 arxivid = {1803.03367},
 author = {Kiar, Gregory and Anderson, Robert J. and Baden, Alex and Badea, Alexandra and Bridgeford, Eric W. and Champion, Andrew and Chandrashekhar, Vikram and Collman, Forrest and Duderstadt, Brandon and Evans, Alan C. and Engert, Florian and Falk, Benjamin and Glatard, Tristan and Roncal, William R. Gray and Kennedy, David N. and Maitin-Shepard, Jeremy and Marren, Ryan A. and Nnaemeka, Onyeka and Perlman, Eric and Seshamani, Sharmishtaas and Trautman, Eric T. and Tward, Daniel J. and Vald√©s-Sosa, Pedro Antonio and Wang, Qing and Miller, Michael I. and Burns, Randal and Vogelstein, Joshua T.},
 author+an = {2=trainee;3=trainee;5=trainee;7=trainee;9=trainee;27=highlight},
 eprint = {1803.03367},
 journal = {arXiv},
 month = {mar},
 title = {{NeuroStorm: Accelerating Brain Science Discovery in the Cloud}},
 url = {http://arxiv.org/abs/1803.03367},
 year = {2018}
}

@article{Priebe2017,
 abstract = {We present semiparametric spectral modeling of the complete larval Drosophila mushroom body connectome. Motivated by a thorough exploratory data analysis of the network via Gaussian mixture modeling (GMM) in the adjacency spectral embedding (ASE) representation space, we introduce the latent structure model (LSM) for network modeling and inference. LSM is a generalization of the stochastic block model (SBM) and a special case of the random dot product graph (RDPG) latent position model, and is amenable to semiparametric GMM in the ASE representation space. The resulting connectome code derived via semiparametric GMM composed with ASE captures latent connectome structure and elucidates biologically relevant neuronal properties.},
 archiveprefix = {arXiv},
 arxivid = {1705.03297},
 author = {Priebe, Carey E. and Park, Youngser and Tang, Minh and Athreya, Avanti and Lyzinski, Vince and Vogelstein, Joshua T. and Qin, Yichen and Cocanougher, Ben and Eichler, Katharina and Zlatic, Marta and Cardona, Albert},
 author+an = {6=highlight},
 eprint = {1705.03297},
 journal = {arXiv},
 title = {{Semiparametric spectral modeling of the Drosophila connectome}},
 url = {http://arxiv.org/abs/1705.03297},
 year = {2017}
}

@article{sinha2014automatic,
 archiveprefix = {arXiv},
 arxivid = {arXiv:1404.4800},
 author = {Sinha, A and Roncal, WG and Kasthuri, N},
 eprint = {arXiv:1404.4800},
 journal = {arXiv},
 title = {{Automatic Annotation of Axoplasmic Reticula in Pursuit of Connectomes}},
 url = {http://arxiv.org/abs/1404.4800},
 year = {2014}
}

@article{Zheng2016,
 abstract = {R is one of the most popular programming languages for statistics and machine learning, but the R framework is relatively slow and unable to scale to large datasets. The general approach for speeding up an implementation in R is to implement the algorithms in C or FORTRAN and provide an R wrapper. FlashR takes a different approach: it executes R code in parallel and scales the code beyond memory capacity by utilizing solid-state drives (SSDs) automatically. It provides a small number of generalized operations (GenOps) upon which we reimplement a large number of matrix functions in the R base package. As such, FlashR parallelizes and scales existing R code with little/no modification. To reduce data movement between CPU and SSDs, FlashR evaluates matrix operations lazily, fuses operations at runtime, and uses cache-aware, two-level matrix partitioning. We evaluate FlashR on a variety of machine learning and statistics algorithms on inputs of up to four billion data points. FlashR out-of-core tracks closely the performance of FlashR in-memory. The R code for machine learning algorithms executed in FlashR outperforms the in-memory execution of H2O and Spark MLlib by a factor of 2-10 and outperforms Revolution R Open by more than an order of magnitude.},
 archiveprefix = {arXiv},
 arxivid = {1604.06414},
 author = {Zheng, Da and Mhembere, Disa and Vogelstein, Joshua T. and Priebe, Carey E. and Burns, Randal},
 author+an = {1=trainee;2=trainee;3=highlight},
 eprint = {1604.06414},
 journal = {CoRR, abs/1604.06414},
 title = {{FlashR: R-Programmed Parallel and Scalable Machine Learning using SSDs}},
 url = {http://arxiv.org/abs/1604.06414},
 year = {2017}
}

@article{Zheng2016c,
 abstract = {Many eigensolvers such as ARPACK and Anasazi have been developed to compute eigenvalues of a large sparse matrix. These eigensolvers are limited by the capacity of RAM. They run in memory of a single machine for smaller eigenvalue problems and require the distributed memory for larger problems. In contrast, we develop an SSD-based eigensolver framework called FlashEigen, which extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph with hundreds of millions or even billions of vertices in a single machine. FlashEigen performs sparse matrix multiplication in a semi-external memory fashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in memory. We store the entire vector subspace on SSDs and reduce I/O to improve performance through caching the most recent dense matrix. Our result shows that FlashEigen is able to achieve 40{\%}-60{\%} performance of its in-memory implementation and has performance comparable to the Anasazi eigensolvers on a machine with 48 CPU cores. Furthermore, it is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges. It takes about four hours to compute eight eigenvalues of the billion-node graph using 120 GB memory.},
 archiveprefix = {arXiv},
 arxivid = {1602.01421},
 author = {Zheng, Da and Burns, Randal and Vogelstein, Joshua and Priebe, Carey E. and Szalay, Alexander S.},
 author+an = {1=trainee;3=highlight},
 eprint = {1602.01421},
 journal = {arXiv},
 title = {{An SSD-based eigensolver for spectral analysis on billion-node graphs}},
 url = {http://arxiv.org/abs/1602.01421},
 year = {2016}
}

