name: Randomer Forest
description: Random forest (RF) has been shown to outperform many other classifiers on a variety of datasets, yet its restriction of recursive partitions of the feature space to be axis-aligned is suboptimal in many cases. Several studies have proposed “oblique” decision forest methods to address this limitation. However, these methods either aren’t well-adapted to problems in which the number of irrelevant features are overwhelming, have a time and space complexity significantly greater than RF, are sensitive to data corruption, or require additional hyperparameters to be tuned, rendering training of the classifier more difficult. We have implemented an oblique decision forest method, called Randomer Forest (RerF), which addresses these issues. Rather than subsampling features at each split node, RerF defines new features at each split node via sparse random projections. Furthermore, we allow the option to rank transform the data prior to inducing the forest, which renders the procedure invariant to scale and mitigates the effects of data corruption. We have demonstrated that RerF emprically outperforms RF and Random Rotation Random Forest on several synthetic classification problems and on 114 benchmark datasets, and also have shown that it scales comparably to RF in terms of time and space complexity.

api: 
github: ttomita/RandomerForest
docs: https://github.com/ttomita/RandomerForest/blob/master/example.pdf
manuscript: 
press: 
